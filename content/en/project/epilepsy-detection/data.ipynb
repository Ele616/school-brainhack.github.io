{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c213b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f36dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the age range in a single value\n",
    "def convert_age_range_to_midpoint(age_string):\n",
    "    \n",
    "    if pd.isna(age_string):\n",
    "        return None\n",
    "    \n",
    "    numbers = re.findall(r'\\d+', str(age_string))\n",
    "    \n",
    "    if len(numbers) >= 2:\n",
    "        min_age = int(numbers[0])\n",
    "        max_age = int(numbers[1])\n",
    "        return (min_age + max_age) / 2\n",
    "    elif len(numbers) == 1:\n",
    "        \n",
    "        return float(numbers[0])\n",
    "    else:\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Transform the files Metadata_Controls_Release.csv and Metadata_Release_Anon.csv in the Subject folder in a single dataframe\n",
    "def create_demographics_dataframe():\n",
    "   \n",
    "    controls_df = pd.read_csv(r'.\\Subject\\Metadata_Controls_Release.csv')\n",
    "    release_df = pd.read_csv(r'.\\Subject\\Metadata_Release_Anon.csv')\n",
    "    \n",
    "    \n",
    "    controls_subset = controls_df[['ID', 'Sex', 'Binned_Age_at_Scan']].copy()\n",
    "    release_subset = release_df[['ID', 'Sex', 'Binned_Age_at_Scan']].copy()\n",
    "    \n",
    "    \n",
    "    controls_subset['Age'] = controls_subset['Binned_Age_at_Scan'].apply(convert_age_range_to_midpoint)\n",
    "    release_subset['Age'] = release_subset['Binned_Age_at_Scan'].apply(convert_age_range_to_midpoint)\n",
    "    \n",
    "    \n",
    "    controls_subset = controls_subset[['ID', 'Sex', 'Age']] #We only keep the features in all patients\n",
    "    release_subset = release_subset[['ID', 'Sex', 'Age']]\n",
    "    \n",
    "    \n",
    "    combined_df = pd.concat([controls_subset, release_subset], ignore_index=True)\n",
    "    \n",
    "    combined_df = combined_df.drop_duplicates(subset=['ID'], keep='first')\n",
    "    combined_df = combined_df.sort_values('ID').reset_index(drop=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e67fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe saved as 'combined_demographics.csv'\n"
     ]
    }
   ],
   "source": [
    "demographics_df = create_demographics_dataframe()\n",
    "\n",
    "demographics_df.to_csv('combined_demographics.csv', index=False)\n",
    "print(\"\\nDataframe saved as 'combined_demographics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee60a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a FreeSurfer stats file and return a DataFrame\n",
    "def read_freesurfer_stats(filepath):\n",
    "    \n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('# ColHeaders'):\n",
    "            headers = line.strip().split()[2:]  \n",
    "            data_start_idx = i + 1\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"No column headers found in the file.\")\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    for line in lines[data_start_idx:]:\n",
    "        if not line.startswith('#') and line.strip():  \n",
    "            data.append(line.strip().split())\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "   \n",
    "   \n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_additional_data(csv_filepath, subject_id_column='Subject_ID'):\n",
    "    \"\"\"\n",
    "    Load additional patient data from CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_filepath : str\n",
    "        Path to the CSV file containing additional patient data\n",
    "    subject_id_column : str\n",
    "        Name of the column containing subject IDs in the CSV\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with additional patient data, indexed by subject ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        additional_data = pd.read_csv(csv_filepath)\n",
    "        \n",
    "        # Convert subject ID column to string to match FreeSurfer data\n",
    "        additional_data[subject_id_column] = additional_data[subject_id_column].astype(str)\n",
    "        \n",
    "        # Set subject ID as index\n",
    "        additional_data.set_index(subject_id_column, inplace=True)\n",
    "        \n",
    "        print(f\"Successfully loaded additional data from: {csv_filepath}\")\n",
    "        print(f\"Additional data shape: {additional_data.shape}\")\n",
    "        print(f\"Additional data columns: {list(additional_data.columns)}\")\n",
    "        \n",
    "        return additional_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading additional data from {csv_filepath}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c014abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lh_aparc_a2009s(base_dir):\n",
    "   \n",
    "    subject_base = os.path.join(base_dir, 'Subject')\n",
    "    \n",
    "    if not os.path.exists(subject_base):\n",
    "        raise ValueError(f\"Subject directory not found: {subject_base}\")\n",
    "    \n",
    "    all_data = {}\n",
    "    processed_subjects = []\n",
    "    \n",
    "    \n",
    "    subject_dirs = [d for d in os.listdir(subject_base) \n",
    "                   if os.path.isdir(os.path.join(subject_base, d)) and d.isdigit()]\n",
    "    \n",
    "    print(f\"Found {len(subject_dirs)} subject directories: {sorted(subject_dirs)}\")\n",
    "    \n",
    "    for subject_id in subject_dirs:\n",
    "        \n",
    "        stats_path = os.path.join(subject_base, subject_id, 'stats', 'lh.aparc.a2009s.stats')\n",
    "        \n",
    "        if not os.path.exists(stats_path):\n",
    "            print(f\"Warning: {stats_path} not found for subject {subject_id}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df = read_freesurfer_stats(stats_path)\n",
    "            \n",
    "            # TO CHANGE TO INCLUDE LESS FEATURES\n",
    "            measure_columns = ['SurfArea', 'GrayVol', 'ThickAvg', 'ThickStd', 'MeanCurv', 'GausCurv', 'FoldInd', 'CurvInd']\n",
    "            \n",
    "            \n",
    "            available_measures = [col for col in measure_columns if col in df.columns]\n",
    "            \n",
    "            if not available_measures:\n",
    "                print(f\"Warning: No standard measures found in {subject_id}\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            subject_data = {}\n",
    "            for _, row in df.iterrows():\n",
    "                region = row['StructName']  \n",
    "                for measure in available_measures:\n",
    "                    column_name = f\"{measure}_{region}\"\n",
    "                    subject_data[column_name] = row[measure]\n",
    "            \n",
    "            all_data[subject_id] = subject_data\n",
    "            processed_subjects.append(subject_id)\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subject {subject_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_data:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "    \n",
    "\n",
    "      \n",
    "    result_df = pd.DataFrame.from_dict(all_data, orient='index')\n",
    "    result_df.index.name = 'Subject'\n",
    "    \n",
    "    \n",
    "    result_df = result_df.reindex(sorted(result_df.index, key=lambda x: int(x)))\n",
    "    \n",
    "    \n",
    "    def get_epileptic_status(subject_id):\n",
    "        subject_num = int(subject_id)\n",
    "        if 1 <= subject_num <= 463:\n",
    "            return 'Epileptic'\n",
    "        elif subject_num >= 4000:\n",
    "            return 'Not Epileptic'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    result_df['Epileptic_Status'] = [get_epileptic_status(subj) for subj in result_df.index]\n",
    "    \n",
    "    cols = ['Epileptic_Status'] + [col for col in result_df.columns if col != 'Epileptic_Status']\n",
    "    result_df = result_df[cols]\n",
    "    \n",
    "    other_cols = sorted([col for col in result_df.columns if col != 'Epileptic_Status'])\n",
    "    result_df = result_df[['Epileptic_Status'] + other_cols]\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(processed_subjects)} subjects\")\n",
    "    print(f\"Final dataset shape: {result_df.shape}\")\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "def process_lh_aparc_a2009s_with_additional_data(base_dir, additional_csv_path=None, \n",
    "                                                subject_id_column='Subject_ID'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    freesurfer_data = process_lh_aparc_a2009s(base_dir)\n",
    "    \n",
    "    if freesurfer_data is None:\n",
    "        print(\"Failed to process FreeSurfer data\")\n",
    "        return None\n",
    "    \n",
    "    # If no additional CSV provided, return original data\n",
    "    if additional_csv_path is None:\n",
    "        print(\"No additional CSV provided, returning FreeSurfer data only\")\n",
    "        return freesurfer_data\n",
    "    \n",
    "    \n",
    "    additional_data = load_additional_data(additional_csv_path, subject_id_column)\n",
    "    \n",
    "    if additional_data is None:\n",
    "        print(\"Failed to load additional data, returning FreeSurfer data only\")\n",
    "        return freesurfer_data\n",
    "    \n",
    "    \n",
    "    print(f\"FreeSurfer data subjects: {sorted(freesurfer_data.index.tolist(), key=int)}\")\n",
    "    print(f\"Additional data subjects: {sorted(additional_data.index.tolist(), key=lambda x: int(x) if x.isdigit() else float('inf'))}\")\n",
    "    \n",
    "    \n",
    "    merged_data = freesurfer_data.join(additional_data, how='left')\n",
    "    \n",
    "    \n",
    "    freesurfer_subjects = set(freesurfer_data.index)\n",
    "    additional_subjects = set(additional_data.index)\n",
    "    \n",
    "    matched_subjects = freesurfer_subjects.intersection(additional_subjects)\n",
    "    freesurfer_only = freesurfer_subjects - additional_subjects\n",
    "    additional_only = additional_subjects - freesurfer_subjects\n",
    "    \n",
    "    print(f\"\\nMerge Results:\")\n",
    "    print(f\"  Subjects with both FreeSurfer and additional data: {len(matched_subjects)}\")\n",
    "    print(f\"  Subjects with only FreeSurfer data: {len(freesurfer_only)}\")\n",
    "    \n",
    "    \n",
    "    if freesurfer_only:\n",
    "        print(f\"  FreeSurfer-only subjects: {sorted(list(freesurfer_only), key=int)}\")\n",
    "    \n",
    "    if additional_only:\n",
    "        print(f\"  Additional-only subjects: {sorted(list(additional_only), key=lambda x: int(x) if x.isdigit() else float('inf'))}\")\n",
    "    \n",
    "    \n",
    "    epileptic_col = ['Epileptic_Status']\n",
    "    additional_cols = [col for col in additional_data.columns if col in merged_data.columns]\n",
    "    freesurfer_cols = [col for col in merged_data.columns \n",
    "                      if col not in epileptic_col and col not in additional_cols]\n",
    "    \n",
    "    final_column_order = epileptic_col + additional_cols + sorted(freesurfer_cols)\n",
    "    merged_data = merged_data[final_column_order]\n",
    "    \n",
    "   \n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "\n",
    "  \n",
    "def process_with_additional_data(): \n",
    "    \n",
    "    base_directory = r\"\"\n",
    "    \n",
    "    # Path to your additional CSV file \n",
    "    additional_csv_path = r'.\\combined_demographics.csv'  # UPDATE THIS PATH\n",
    "    \n",
    "    subject_id_column = 'ID'  # UPDATE THIS IF THE COLUMN HAS A DIFFERENT NAME\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        combined_data = process_lh_aparc_a2009s_with_additional_data(\n",
    "            base_directory, \n",
    "            additional_csv_path, \n",
    "            subject_id_column\n",
    "        )\n",
    "        \n",
    "        if combined_data is not None:\n",
    "           \n",
    "         \n",
    "            epileptic_cols = [col for col in combined_data.columns if 'Epileptic' in col]\n",
    "            additional_cols = [col for col in combined_data.columns \n",
    "                             if col not in epileptic_cols and not any(measure in col for measure in \n",
    "                             ['SurfArea', 'GrayVol', 'ThickAvg', 'ThickStd', 'MeanCurv', 'GausCurv', 'FoldInd', 'CurvInd'])]\n",
    "            freesurfer_cols = [col for col in combined_data.columns \n",
    "                             if col not in epileptic_cols and col not in additional_cols]\n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            output_file = 'lh.csv'\n",
    "            combined_data.to_csv(output_file)\n",
    "            print(f\"\\nCombined data saved to: {output_file}\")\n",
    "            \n",
    "            \n",
    "            missing_summary = combined_data.isnull().sum()\n",
    "            cols_with_missing = missing_summary[missing_summary > 0]\n",
    "            \n",
    "            if len(cols_with_missing) > 0:\n",
    "                print(f\"\\nColumns with missing values:\")\n",
    "                for col, count in cols_with_missing.items():\n",
    "                    percentage = (count / len(combined_data)) * 100\n",
    "                    print(f\"  {col}: {count} missing ({percentage:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"\\nNo missing values in the dataset!\")\n",
    "            \n",
    "            return combined_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d12d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 542 subject directories: ['1', '10', '100', '101', '102', '103', '104', '105', '106', '108', '109', '11', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '12', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '13', '130', '131', '132', '133', '134', '135', '136', '137', '138', '14', '141', '142', '143', '144', '145', '146', '147', '148', '149', '15', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '16', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '17', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '18', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '19', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '2', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '21', '210', '211', '213', '214', '215', '216', '217', '218', '219', '22', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '23', '230', '231', '232', '233', '234', '235', '236', '237', '238', '24', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '25', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '26', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '27', '270', '271', '273', '275', '276', '277', '278', '279', '28', '280', '281', '282', '283', '285', '286', '287', '288', '289', '29', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '3', '30', '300', '301', '302', '303', '304', '305', '306', '308', '309', '31', '310', '311', '312', '313', '314', '315', '316', '317', '319', '32', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '33', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '34', '340', '341', '343', '344', '345', '346', '347', '348', '349', '35', '350', '351', '352', '353', '354', '355', '356', '357', '358', '36', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '37', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '38', '380', '381', '382', '383', '384', '385', '387', '388', '389', '39', '390', '391', '392', '393', '394', '395', '396', '397', '399', '4', '40', '400', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '401', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '402', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '403', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '404', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '405', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '406', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '407', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '408', '4080', '4081', '4082', '4083', '4084', '4085', '4086', '4087', '4088', '4089', '409', '4090', '4091', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '41', '410', '4100', '411', '412', '413', '414', '415', '416', '417', '418', '419', '42', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '43', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '44', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '45', '450', '451', '452', '453', '454', '455', '459', '46', '460', '461', '462', '463', '47', '48', '49', '5', '50', '51', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '74', '75', '76', '77', '78', '79', '8', '81', '82', '83', '84', '85', '86', '87', '88', '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toulo\\AppData\\Local\\Temp\\ipykernel_18728\\3929357964.py:26: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[col] = pd.to_numeric(df[col], errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 542 subjects\n",
      "Final dataset shape: (542, 593)\n",
      "Successfully loaded additional data from: .\\combined_demographics.csv\n",
      "Additional data shape: (542, 2)\n",
      "Additional data columns: ['Sex', 'Age']\n",
      "FreeSurfer data subjects: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '77', '78', '79', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '273', '275', '276', '277', '278', '279', '280', '281', '282', '283', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '459', '460', '461', '462', '463', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '4080', '4081', '4082', '4083', '4084', '4085', '4086', '4087', '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '4100']\n",
      "Additional data subjects: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '74', '75', '76', '77', '78', '79', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '273', '275', '276', '277', '278', '279', '280', '281', '282', '283', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '459', '460', '461', '462', '463', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '4080', '4081', '4082', '4083', '4084', '4085', '4086', '4087', '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '4100']\n",
      "\n",
      "Merge Results:\n",
      "  Subjects with both FreeSurfer and additional data: 542\n",
      "  Subjects with only FreeSurfer data: 0\n",
      "\n",
      "Combined data saved to: lh.csv\n",
      "\n",
      "Columns with missing values:\n",
      "  CurvInd_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  FoldInd_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  GausCurv_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  GrayVol_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  MeanCurv_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  SurfArea_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  ThickAvg_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "  ThickStd_G_cingul-Post-ventral: 1 missing (0.2%)\n",
      "        Epileptic_Status Sex   Age  CurvInd_G_Ins_lg_and_S_cent_ins  \\\n",
      "Subject                                                               \n",
      "1              Epileptic   M  22.0                              0.8   \n",
      "2              Epileptic   F  37.0                              0.6   \n",
      "3              Epileptic   M  27.0                              1.4   \n",
      "4              Epileptic   M  42.0                              1.2   \n",
      "5              Epileptic   F  47.0                              0.7   \n",
      "...                  ...  ..   ...                              ...   \n",
      "4096       Not Epileptic   F  32.0                              0.9   \n",
      "4097       Not Epileptic   M  37.0                              1.4   \n",
      "4098       Not Epileptic   F  27.0                              1.0   \n",
      "4099       Not Epileptic   M  27.0                              0.9   \n",
      "4100       Not Epileptic   F  37.0                              1.1   \n",
      "\n",
      "         CurvInd_G_and_S_cingul-Ant  CurvInd_G_and_S_cingul-Mid-Ant  \\\n",
      "Subject                                                               \n",
      "1                               2.4                             0.9   \n",
      "2                               2.4                             1.1   \n",
      "3                               2.1                             0.7   \n",
      "4                               1.9                             0.9   \n",
      "5                               2.6                             1.1   \n",
      "...                             ...                             ...   \n",
      "4096                            3.0                             1.1   \n",
      "4097                            3.2                             0.8   \n",
      "4098                            1.7                             1.1   \n",
      "4099                            2.4                             0.9   \n",
      "4100                            1.8                             0.9   \n",
      "\n",
      "         CurvInd_G_and_S_cingul-Mid-Post  CurvInd_G_and_S_frontomargin  \\\n",
      "Subject                                                                  \n",
      "1                                    1.7                           2.0   \n",
      "2                                    1.3                           1.2   \n",
      "3                                    0.9                           1.1   \n",
      "4                                    0.9                           1.7   \n",
      "5                                    0.9                           1.9   \n",
      "...                                  ...                           ...   \n",
      "4096                                 1.1                           1.2   \n",
      "4097                                 1.1                           1.8   \n",
      "4098                                 1.1                           1.5   \n",
      "4099                                 1.1                           2.2   \n",
      "4100                                 0.9                           1.8   \n",
      "\n",
      "         CurvInd_G_and_S_occipital_inf  CurvInd_G_and_S_paracentral  ...  \\\n",
      "Subject                                                              ...   \n",
      "1                                  2.1                          1.9  ...   \n",
      "2                                  1.6                          1.9  ...   \n",
      "3                                  2.7                          2.0  ...   \n",
      "4                                  2.1                          2.3  ...   \n",
      "5                                  1.6                          2.2  ...   \n",
      "...                                ...                          ...  ...   \n",
      "4096                               2.0                          1.4  ...   \n",
      "4097                               1.7                          2.7  ...   \n",
      "4098                               1.7                          1.3  ...   \n",
      "4099                               1.5                          1.4  ...   \n",
      "4100                               1.5                          1.2  ...   \n",
      "\n",
      "         ThickStd_S_parieto_occipital  ThickStd_S_pericallosal  \\\n",
      "Subject                                                          \n",
      "1                               0.598                    0.556   \n",
      "2                               0.515                    0.579   \n",
      "3                               0.545                    0.725   \n",
      "4                               0.650                    0.611   \n",
      "5                               0.533                    0.641   \n",
      "...                               ...                      ...   \n",
      "4096                            0.409                    0.602   \n",
      "4097                            0.510                    0.464   \n",
      "4098                            0.476                    0.613   \n",
      "4099                            0.478                    0.518   \n",
      "4100                            0.462                    0.638   \n",
      "\n",
      "         ThickStd_S_postcentral  ThickStd_S_precentral-inf-part  \\\n",
      "Subject                                                           \n",
      "1                         0.413                           0.344   \n",
      "2                         0.421                           0.473   \n",
      "3                         0.473                           0.404   \n",
      "4                         0.597                           0.465   \n",
      "5                         0.415                           0.413   \n",
      "...                         ...                             ...   \n",
      "4096                      0.401                           0.414   \n",
      "4097                      0.559                           0.392   \n",
      "4098                      0.421                           0.415   \n",
      "4099                      0.497                           0.354   \n",
      "4100                      0.461                           0.421   \n",
      "\n",
      "         ThickStd_S_precentral-sup-part  ThickStd_S_suborbital  \\\n",
      "Subject                                                          \n",
      "1                                 0.341                  0.810   \n",
      "2                                 0.396                  0.721   \n",
      "3                                 0.407                  0.604   \n",
      "4                                 0.541                  0.680   \n",
      "5                                 0.374                  0.809   \n",
      "...                                 ...                    ...   \n",
      "4096                              0.413                  0.514   \n",
      "4097                              0.384                  0.542   \n",
      "4098                              0.396                  0.795   \n",
      "4099                              0.365                  0.692   \n",
      "4100                              0.458                  0.637   \n",
      "\n",
      "         ThickStd_S_subparietal  ThickStd_S_temporal_inf  \\\n",
      "Subject                                                    \n",
      "1                         0.436                    0.450   \n",
      "2                         0.689                    0.514   \n",
      "3                         0.390                    0.776   \n",
      "4                         0.697                    0.602   \n",
      "5                         0.349                    0.462   \n",
      "...                         ...                      ...   \n",
      "4096                      0.466                    0.535   \n",
      "4097                      0.517                    0.370   \n",
      "4098                      0.358                    0.410   \n",
      "4099                      0.485                    0.461   \n",
      "4100                      0.584                    0.470   \n",
      "\n",
      "         ThickStd_S_temporal_sup  ThickStd_S_temporal_transverse  \n",
      "Subject                                                           \n",
      "1                          0.475                           0.403  \n",
      "2                          0.580                           0.304  \n",
      "3                          0.634                           0.461  \n",
      "4                          0.498                           0.344  \n",
      "5                          0.470                           0.463  \n",
      "...                          ...                             ...  \n",
      "4096                       0.483                           0.331  \n",
      "4097                       0.514                           0.761  \n",
      "4098                       0.472                           0.569  \n",
      "4099                       0.418                           0.383  \n",
      "4100                       0.423                           0.580  \n",
      "\n",
      "[542 rows x 595 columns]\n"
     ]
    }
   ],
   "source": [
    "data = process_with_additional_data()\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
